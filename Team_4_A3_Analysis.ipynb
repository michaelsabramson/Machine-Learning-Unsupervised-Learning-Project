{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h1>A3 Analysis - Unsupervised Learning </h1>\n",
    "<h2>Machine Learning - DAT-5303 - FMSBA3 </h2>\n",
    "By: Sophie Briques, Michael Abramson, Yevheniya Boyko, Rakesh Joe Francy, Junjie Huang, Santiago Romero<br>\n",
    "Hult International Business School<br><br><br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Survey data from Big 5 personality traits and Hult DNA to understand purchase behavior or Windows and Macbook users\n",
    "- Unsupervised learning technique PCA was used to uncover psychometrics of the target audience\n",
    "- Unsupervised learning technique k-Means clustering was used to determine similar groups of customers\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Case: Microsoft </strong> <br>\n",
    "<i> Audience: Microsoft Analytics Team </i> <br>\n",
    "<strong> Goal: </strong> understand consumer buying behavior in regards to the decision making behind choosing Mac or Windows through the lenses of Big Five Personalities and Hult DNA<br>\n",
    "<strong> Source: </strong> survey conducted through Google Forms on 245 individuals in February 2020\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Data and Assumptions: </strong> <br><br>\n",
    "<u>Dataset:</u><br>\n",
    "The dataset in this script comes from a survey conducted in February 2020 to students at Hult International Business School. Each survey answer corresponded to a range between 1 to 5, where 1 corresponded to strongly disagree and 5 to strongly agree and where 3 represents a neutral feeling. There are 245 respondents.\n",
    "<br>\n",
    "\n",
    "<u>Specifications: </u> \n",
    "- audience surveyed is representative of the population Microsoft is attempting to study\n",
    "- audience surveyed is composed solely of Hult students\n",
    "\n",
    "<u>Assumptions: </u>\n",
    "- Confirmatory factor analysis: pre-checks are completed\n",
    "- All survey respondents have are fluent in English\n",
    "- No respondents were aware of the Big Five Personalities previous to completing the survey\n",
    "- All respondents identify themselves as female or male solely\n",
    "\n",
    "<u> Demographic Data: </u> <br>\n",
    "- age\n",
    "- nationality\n",
    "- gender\n",
    "- ethnicity\n",
    "\n",
    "<u> Spending Behavior data: </u> <br>\n",
    "- Current laptop spending\n",
    "- Ideal new laptop (if all prices were the same)\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Null Hypothesis: </strong> <br><br>\n",
    "<u> Big 5 Personalities </u> <br>\n",
    "The survey conducted comes from the <a href=\"https://doi.org/10.1111/j.1744-6570.1999.tb00174.x\">Big Five personality traits</a> which was a survey conducted on American population to determine 5 overarching personality traits. In this case, the personalities are being analyzed in relation to the customers buying behavior:\n",
    "\n",
    "\n",
    "1. Agreeableness is a measure of ones tendency towards social harmony. In other words, the level of cooperation and team interaction is scored.\n",
    "2. Conscientiousness is a measure of self-discipline and individual level of productivity displayed.\n",
    "3. Extraversion represents how social and energetic an individual tends to be. The measure is also used to identify a person as either introvert or an extrovert. \n",
    "4. Openness measures the extent of imagination and creativity displayed, contrary to a rather conventional personality.\n",
    "5. Stress Tolerance represents ones ability to handle a stressful situation.\n",
    "\n",
    "The approach adapted in the present study intends to analyze and present the insights of the role the Big 5 Traits play towards the overall performance of Microsoft. <br><br>\n",
    "We expect that due to the diverse and international nature of all respondents, we might see one or multiple different traits emerge as opposed to the original 5 traits.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Hult DNA </u> <br>\n",
    "<a href = \"https://www.hult.edu/blog/why-every-leader-needs-growth-mindset/\"> Hult DNA </a>  is a combination of cognitive-behavioral skills, consisting of the following categories: \n",
    "1. Thinking reflects students’ ability recognize and challenge their bias towards a fixed mindset. \n",
    "2. Communicating category reflects the students’ ability to recognize their weak and strong points and communicate those in a proper manner.\n",
    "3. Team Building skill is a foothold of the growth mindset, representing the ability of a team-member to leverage cultural differences in pursue of mutual understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Outline: </strong>\n",
    "0. Preparation\n",
    "1. Part 1: Anomaly Detection and Handling\n",
    "2. Part 2: Exploratory Data Analysis\n",
    "3. Part 3: Transformations\n",
    "4. Part 4: Build an unsupervised learning model\n",
    "5. Part 5: Interpreting the Model (Correlations and Clustering)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to import necessary packages, load data, and set display options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary Libraries\n",
    "import pandas            as pd  # data science essentials\n",
    "import matplotlib.pyplot as plt # fundamental data visualization\n",
    "import seaborn           as sns # enhanced visualization\n",
    "import sys                      # system-specific parameters and functions\n",
    "from sklearn.preprocessing   import StandardScaler # standard scaler\n",
    "from sklearn.decomposition   import PCA            # pca\n",
    "from sklearn.manifold        import TSNE           # t-SNE\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage # dendrograms\n",
    "from sklearn.cluster         import KMeans              # k-means clustering\n",
    "\n",
    "# setting print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# loading the data\n",
    "file    = 'Survey_Data_Final_Exam.xlsx'\n",
    "full_df = pd.read_excel(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Run the following dictionary to separate survey questions and answers according to survey source and question type. <br><br>\n",
    "\n",
    "<i> Note: This dictionary also removes duplicate questions from the full dataset. Some questions were duplicated to be able to identify if respondents were responding seriously as opposed to randomly selecting answers.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "   'negative': [\n",
    "       \"Don't talk a lot\", \n",
    "       'Am not interested in abstract ideas',\n",
    "       \"Am not interested in other people's problems\",\n",
    "       'Do not have a good imagination',\n",
    "       'Am not really interested in others',\n",
    "       \"Don't like to draw attention to myself\",\n",
    "       \"Don't mind being the center of attention\",\n",
    "       \"Don't  generate ideas that are new and different\",\n",
    "       \"Don't persuasively sell a vision or idea\",\n",
    "       \"Can't rally people on the team around a common goal\"\n",
    "    ],\n",
    "    'bigfive': [\n",
    "        'Am the life of the party',           'Feel little concern for others', \n",
    "        'Am always prepared',                 'Get stressed out easily', \n",
    "        'Have a rich vocabulary',             \"Talk a lot\", \n",
    "        'Am interested in people',            'Leave my belongings around', \n",
    "        'Am relaxed most of the time',        'Have difficulty understanding abstract ideas',\n",
    "        'Feel comfortable around people',     'Insult people',\n",
    "        'Pay attention to details',           'Worry about things',\n",
    "        'Have a vivid imagination',           'Keep in the background', \n",
    "        \"Sympathize with others' feelings\",   'Make a mess of things', \n",
    "        'Seldom feel blue',                   'Interested in abstract ideas',\n",
    "        'Start conversations',                \"Interested in other people's problems\", \n",
    "        'Get chores done right away',         'Am easily disturbed',\n",
    "        'Have excellent ideas',               'Have little to say',\n",
    "        'Have a soft heart',                  'Often forget to put things back in their proper place',\n",
    "        'Get upset easily',                   'Have a good imagination',\n",
    "        'Really interested in others',        'Talk to a lot of different people at parties',\n",
    "        'Like order',                         'Change my mood a lot',\n",
    "        'Am quick to understand things',      \"Like to draw attention to myself\",\n",
    "        'Take time out for others',           'Shirk my duties',\n",
    "        'Have frequent mood swings',          'Use difficult words',\n",
    "        \"Feel others' emotions\",              'Follow a schedule',\n",
    "        'Get irritated easily',               \"Mind being the center of attention\",\n",
    "        'Spend time reflecting on things',    'Am quiet around strangers',\n",
    "        'Make people feel at ease',           'Am exacting in my work', \n",
    "        'Often feel blue',                    'Am full of ideas'\n",
    "    ],\n",
    "    'hultdna': [\n",
    "        'See underlying patterns in complex situations', \n",
    "        \"Generate ideas that are new and different\",\n",
    "        'Demonstrate an awareness of personal strengths and limitations',\n",
    "        'Display a growth mindset', \n",
    "        'Respond effectively to multiple priorities',\n",
    "        \"Take initiative even when circumstances, objectives, or rules aren't clear\", \n",
    "        'Encourage direct and open discussions', \n",
    "        #'Respond effectively to multiple priorities.1', \n",
    "        #\"Take initiative even when circumstances, objectives, or rules aren't clear.1\",\n",
    "        #'Encourage direct and open discussions.1', \n",
    "        'Listen carefully to others',\n",
    "        \"Persuasively sell a vision or idea\",\n",
    "        'Build cooperative relationships',\n",
    "        'Work well with people from diverse cultural backgrounds',\n",
    "        'Effectively negotiate interests, resources, and roles', \n",
    "        \"Can rally people on the team around a common goal\",\n",
    "        'Translate ideas into plans that are organized and realistic', \n",
    "        'Resolve conflicts constructively',\n",
    "        'Seek and use feedback from teammates',\n",
    "        'Coach teammates for performance and growth', \n",
    "        'Drive for results'\n",
    "    ],\n",
    "    'demographic' : [\n",
    "        'What laptop do you currently have?',\n",
    "        'What laptop would you buy in next assuming if all laptops cost the same?',\n",
    "        'What program are you in?',\n",
    "        'What is your age?',\n",
    "        'Gender',\n",
    "        'What is your nationality? ',\n",
    "        'What is your ethnicity?'\n",
    "    ],\n",
    "    'corr' : [\n",
    "        'A', 'E', \n",
    "        'N', 'C', 'O',\n",
    "        'What laptop do you currently have?',\n",
    "        'What laptop would you buy in next assuming if all laptops cost the same?'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-defined Functions\n",
    "\n",
    "Run the following code to load user-defined functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Defining a function to standardize numerical variables in the dataset:\n",
    "def standard(num_df, by_rows = False):\n",
    "    \"\"\"\n",
    "    This function standardizes a dataframe that contains variables which are either\n",
    "    integers or floats.\n",
    "    \n",
    "    ------\n",
    "    num_df  : DataFrame, must contain only numerical variables\n",
    "    by_rows : bool, Default = False. If True, will standardize DataFrame per rows\n",
    "    \n",
    "    \"\"\"\n",
    "    # INSTANTIATING a StandardScaler() object\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    if by_rows == False:\n",
    "        # FITTING the scaler\n",
    "        scaler.fit(num_df)\n",
    "    \n",
    "        # TRANSFORMING our data after fit\n",
    "        scaled = scaler.transform(num_df)\n",
    "    \n",
    "        # converting scaled data into a DataFrame\n",
    "        scaled_df = pd.DataFrame(scaled)\n",
    "        \n",
    "        # adding labels to the scaled DataFrame\n",
    "        scaled_df.columns = num_df.columns\n",
    "        \n",
    "        # returning the standardized data frame into the global environment\n",
    "        return scaled_df\n",
    "    \n",
    "    elif by_rows == True:\n",
    "        \n",
    "        # Transposing data frame\n",
    "        transpose_df = num_df.transpose()\n",
    "        \n",
    "        # FITTING the scaler\n",
    "        scaler.fit(transpose_df)\n",
    "    \n",
    "        # TRANSFORMING our data after fit\n",
    "        transposed_scaled = scaler.transform(transpose_df)\n",
    "        \n",
    "        # Re-transposing our data frame \n",
    "        scaled = transposed_scaled.transpose()\n",
    "        \n",
    "        # converting scaled data into a DataFrame\n",
    "        scaled_df = pd.DataFrame(scaled)\n",
    "        \n",
    "        # adding labels to the scaled DataFrame\n",
    "        scaled_df.columns = num_df.columns\n",
    "        \n",
    "        # returning the standardized data frame into the global environment\n",
    "        return scaled_df\n",
    "    \n",
    "    else:\n",
    "        print('Something went wrong. Please specifiy by_rows argument as True or False.')\n",
    "        \n",
    "        \n",
    "########################################\n",
    "# scree_plot\n",
    "########################################\n",
    "def scree_plot(pca_object, export = False):\n",
    "    # building a scree plot\n",
    "\n",
    "    # setting plot size\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    features = range(pca_object.n_components_)\n",
    "\n",
    "\n",
    "    # developing a scree plot\n",
    "    plt.plot(features,\n",
    "             pca_object.explained_variance_ratio_,\n",
    "             linewidth = 2,\n",
    "             marker = 'o',\n",
    "             markersize = 10,\n",
    "             markeredgecolor = 'black',\n",
    "             markerfacecolor = 'grey')\n",
    "\n",
    "\n",
    "    # setting more plot options\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('PCA feature')\n",
    "    plt.ylabel('Explained Variance')\n",
    "    plt.xticks(features)\n",
    "\n",
    "    if export == True:\n",
    "    \n",
    "        # exporting the plot\n",
    "        plt.savefig('top_customers_correlation_scree_plot.png')\n",
    "        \n",
    "    # displaying the plot\n",
    "    plt.show()\n",
    "    \n",
    "########################################\n",
    "# inertia\n",
    "########################################\n",
    "def interia_plot(data, max_clust = 50):\n",
    "    \"\"\"\n",
    "PARAMETERS\n",
    "----------\n",
    "data      : DataFrame, data from which to build clusters. Dataset should be scaled\n",
    "max_clust : int, maximum of range for how many clusters to check interia, default 50\n",
    "    \"\"\"\n",
    "\n",
    "    ks = range(1, max_clust)\n",
    "    inertias = []\n",
    "\n",
    "\n",
    "    for k in ks:\n",
    "        # INSTANTIATING a kmeans object\n",
    "        model = KMeans(n_clusters = k)\n",
    "\n",
    "\n",
    "        # FITTING to the data\n",
    "        model.fit(data)\n",
    "\n",
    "\n",
    "        # append each inertia to the list of inertias\n",
    "        inertias.append(model.inertia_)\n",
    "\n",
    "\n",
    "\n",
    "    # plotting ks vs inertias\n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    plt.plot(ks, inertias, '-o')\n",
    "\n",
    "\n",
    "    # labeling and displaying the plot\n",
    "    plt.xlabel('number of clusters, k')\n",
    "    plt.ylabel('inertia')\n",
    "    plt.xticks(ks)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Anomaly Detection and Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Reversed Answer Scales: </strong>\n",
    "<br><br>\n",
    "Questions in the survey range from 1 to 5, where 5 corresponds to strongly agree and 1 to strongly disagree.\n",
    "However, certain questions are phrased in a negative conotation, reversing the above scale. <br>\n",
    "\n",
    "For example: In the question <i> \"Don't talk a lot\" </i>, a 5 would be someone who is very quiet. <br>\n",
    "\n",
    "This represents an issue in our analysis, since we are not able to directly compare these questions. We will therefore reverse the scoring for the 'negative' questions, so that <i> \"Don't talk a lot\" </i> becomes <i> \"Talk a lot\" </i> and where a previous 5 (very quiet) becomes a 1 with the same meaning (very quiet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reversing answer scales\n",
    "full_df[dictionary['negative']] = full_df[dictionary['negative']].replace(to_replace = [5,4,2,1],\n",
    "                                                                          value=[1,2,4,5])\n",
    "\n",
    "\n",
    "positive = [\"Talk a lot\",\n",
    "            'Interested in abstract ideas',\n",
    "            \"Interested in other people's problems\",\n",
    "            'Have a good imagination',\n",
    "            'Really interested in others',\n",
    "            \"Like to draw attention to myself\",\n",
    "            \"Mind being the center of attention\",\n",
    "            \"Generate ideas that are new and different\",\n",
    "            \"Persuasively sell a vision or idea\",\n",
    "            \"Can rally people on the team around a common goal\"]\n",
    "\n",
    "# Renaming column questions with positive conotations\n",
    "full_df.rename(columns=dict(zip(dictionary['negative'], positive)), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<strong> Duplicates: <strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we analyse the survey, we identified multiple duplicate observations. These observations had the exact same answers, including text entry for age and nationalities. However, their survey ID was different. We decided to remove these from our dataset to avoid enflating a certain personality trait over any other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Removing Survey ID column to be able to identify duplicates\n",
    "full_df0 = full_df.drop('surveyID', axis = 1)\n",
    "\n",
    "# Dropping duplicates from our dataset\n",
    "full_df_noduplicates = full_df0.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Data Cleaning: </strong> Nationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question that asked respondents what were their nationalities was set up so that respondents could enter free short-text. Even though this allowed respondents to answer with specific cases (ex: double nationalities), we also need to make sure each answer follows the same formatting to be able to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#Nationality\n",
    "#change all to lowercase\n",
    "full_df_noduplicates.loc[:,'What is your nationality? '] = full_df_noduplicates.loc[:,'What is your nationality? '].str.lower()\n",
    "\n",
    "# Standardizing country formatting \n",
    "full_df_noduplicates.loc[:,'What is your nationality? '] = full_df_noduplicates.loc[:,'What is your nationality? '].replace(\n",
    "                                                            to_replace = ['india',             'china',          'korea',\n",
    "                                                                          'korean',            'ecuador',        'taiwan',\n",
    "                                                                          'usa',               'japan',          'russia',\n",
    "                                                                          'spain',             'brazil',         'colombia',\n",
    "                                                                          'republic of korea', 'costarrican',    'mauritius',\n",
    "                                                                          'cameroon',          'indonesia',      'panama',\n",
    "                                                                          'germany',           'czech republic', 'nigeria',\n",
    "                                                                          'canada',            'belgian',        'south korea',\n",
    "                                                                          'venezuela',         'congolese (dr congo)',\n",
    "                                                                          'mexico',            'armenia',         'thailand',\n",
    "                                                                          'dominican republic','philippines',     'peru',\n",
    "                                                                          'indian.',           'malaysia',        'iran',\n",
    "                                                                          'english',           'el salvador',     'belarus',\n",
    "                                                                          'taiwan( r.o.c)',    'costa rica'\n",
    "                                                                         ],\n",
    "                                                            value      = ['indian',       'chinese',     'south korean',\n",
    "                                                                          'south korean', 'ecuadorian',  'taiwanese',\n",
    "                                                                          'american',     'japanese',    'russian',\n",
    "                                                                          'spanish',      'brazilian',   'colombian',\n",
    "                                                                          'south korean', 'costa rican', 'mauritian',\n",
    "                                                                          'cameroonian',  'indonesian',  'panamanian',\n",
    "                                                                          'german',       'czech',       'nigerian',\n",
    "                                                                          'canadian',     'belgian',     'south korean',\n",
    "                                                                          'venezuelan',   'congolese',   'mexican',\n",
    "                                                                          'armenian',     'thai',        'dominican',\n",
    "                                                                          'filipino',     'peruvian',    'indian',\n",
    "                                                                          'malaysian',    'iranian',     'british',\n",
    "                                                                          'salvadoran',   'belarusian',  'taiwanese',\n",
    "                                                                          'costa rican'\n",
    "                                                                         ])\n",
    "\n",
    "# Grouping unknown nationalities\n",
    "full_df_noduplicates.loc[:,'What is your nationality? '] = full_df_noduplicates.loc[:,'What is your nationality? '].replace(\n",
    "                                                             to_replace = ['.', 'calm', 'multi-ethnic',\n",
    "                                                                           'prefer not to answer'\n",
    "                                                                          ],\n",
    "                                                              value     = ['unknown','unknown','unknown',\n",
    "                                                                           'unknown'\n",
    "                                                                          ])\n",
    "# Checking results\n",
    "#full_df_noduplicates['What is your nationality? '].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<strong> Data Cleaning: </strong> Laptop Used\n",
    "<br>\n",
    "One respondent used the text entry for computers to answer the question about their current laptop. We will group this observation with the corresponding category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Replacing MAC as Macbook\n",
    "full_df_noduplicates.loc[: , 'What laptop do you currently have?'] = full_df_noduplicates.loc[: , 'What laptop do you currently have?'].replace(\n",
    "                                                                        to_replace = 'MAC',\n",
    "                                                                        value = 'Macbook')\n",
    "\n",
    "\n",
    "full_df_noduplicates.loc[: ,'What laptop would you buy in next assuming if all laptops cost the same?'] = full_df_noduplicates.loc[: ,'What laptop would you buy in next assuming if all laptops cost the same?'].replace(\n",
    "                                                                        to_replace = 'MAC',\n",
    "                                                                        value = 'Macbook')\n",
    "\n",
    "\n",
    "#full_df_noduplicates.loc[: , 'What laptop do you currently have?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<strong> Missing Values: </strong> Ethnicity <br>\n",
    "One missing value but all others are aligned. We will impute the missing value with 'Prefer not to answer'. Due to the sensitivity of this question, we assume that a person that did not answer this question preferred not to answer it at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values\n",
    "full_df_noduplicates.loc[:,'What is your ethnicity?'] = full_df_noduplicates.loc[:,'What is your ethnicity?'].fillna('Prefer not to answer')\n",
    "\n",
    "# Checking our results\n",
    "#full_df_noduplicates.loc[:,'What is your ethnicity?'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<strong> Correlation Analysis: </strong> <br>\n",
    "After completing the Big Five personality survey, the following formulas are used to calculate a score for each of the personalities, where $qt_{\\substack{n}}$ is the survey question number $n$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = 20 + qt_{\\substack{1}} + qt_{\\substack{6}} + qt_{\\substack{11}} + qt_{\\substack{16}} + qt_{\\substack{21}} + qt_{\\substack{26}} + qt_{\\substack{31}} + qt_{\\substack{36}} + qt_{\\substack{41}} + qt_{\\substack{46}} $$\n",
    "\n",
    "$$ A = 14 - qt_{\\substack{2}} + qt_{\\substack{7}} + qt_{\\substack{12}} + qt_{\\substack{17}} + qt_{\\substack{22}} + qt_{\\substack{27}} + qt_{\\substack{32}} + qt_{\\substack{37}} + qt_{\\substack{42}} + qt_{\\substack{47}} $$\n",
    "\n",
    "\n",
    "$$ C = 14 + qt_{\\substack{3}} + qt_{\\substack{8}} + qt_{\\substack{13}} + qt_{\\substack{18}} + qt_{\\substack{23}} + qt_{\\substack{28}} + qt_{\\substack{33}} + qt_{\\substack{38}} + qt_{\\substack{43}} + qt_{\\substack{48}} $$\n",
    "\n",
    "\n",
    "$$ N = 2 - qt_{\\substack{4}} + qt_{\\substack{9}} + qt_{\\substack{14}} + qt_{\\substack{19}} + qt_{\\substack{24}} + qt_{\\substack{29}} + qt_{\\substack{34}} + qt_{\\substack{39}} + qt_{\\substack{44}} + qt_{\\substack{49}} $$\n",
    "\n",
    "\n",
    "$$ O = 8 + qt_{\\substack{5}} + qt_{\\substack{10}} + qt_{\\substack{155}} + qt_{\\substack{20}} + qt_{\\substack{25}} + qt_{\\substack{30}} + qt_{\\substack{35}} + qt_{\\substack{40}} + qt_{\\substack{45}} + qt_{\\substack{50}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://openpsychometrics.org/printable/big-five-personality-test.pdf\"> Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Note: We noticed that their equation for our source's neurotic personality score seemed to be reversed. It was subtracting points for neurotic where it should have been adding points. To fix this, we reversed the signs associated with each question and inverted the intercept in relation to the mean.<br><br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing our own personality test, we want to compute the scores for each individual, in order to determine their scores for each category they would belong to in the traditional test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computing scores based on original Big 5 formula\n",
    "full_df = full_df_noduplicates.copy()\n",
    "\n",
    "# Extraversion\n",
    "full_df['E'] = (20 + full_df['Am the life of the party']\n",
    "                   + full_df['Talk a lot'] # reversed the sign because of reversed scale\n",
    "                   - full_df['Keep in the background']\n",
    "                   + full_df['Feel comfortable around people']\n",
    "                   + full_df['Start conversations']\n",
    "                   - full_df['Have little to say']\n",
    "                   + full_df['Talk to a lot of different people at parties']\n",
    "                   + full_df['Like to draw attention to myself']   # reversed the sign because of reversed scale\n",
    "                   - full_df['Mind being the center of attention'] # reversed the sign because of reversed scale\n",
    "                   - full_df['Am quiet around strangers'] \n",
    "               )\n",
    "\n",
    "# Neuroticism\n",
    "full_df['N'] = (2 + full_df['Get stressed out easily']\n",
    "                  -  full_df['Am relaxed most of the time']  \n",
    "                   + full_df['Worry about things']\n",
    "                   - full_df['Seldom feel blue']\n",
    "                   + full_df['Am easily disturbed']\n",
    "                   + full_df['Get upset easily']\n",
    "                   + full_df['Change my mood a lot']\n",
    "                   + full_df['Have frequent mood swings']\n",
    "                   + full_df['Get irritated easily']\n",
    "                   + full_df['Often feel blue']\n",
    "               )\n",
    "\n",
    "# Conscientiousness\n",
    "full_df['C'] = (14 + full_df['Am always prepared'] \n",
    "                   - full_df['Leave my belongings around'] \n",
    "                   + full_df['Pay attention to details'] \n",
    "                   - full_df['Make a mess of things'] \n",
    "                   + full_df['Get chores done right away'] \n",
    "                   - full_df['Often forget to put things back in their proper place']\n",
    "                   + full_df['Like order']\n",
    "                   - full_df['Shirk my duties']\n",
    "                   + full_df['Follow a schedule']\n",
    "                   + full_df['Am exacting in my work'] \n",
    "               )\n",
    "\n",
    "# Agreeableness\n",
    "full_df['A'] = (14 - full_df['Feel little concern for others']\n",
    "                   + full_df['Am interested in people']\n",
    "                   - full_df['Insult people'] \n",
    "                   + full_df[\"Sympathize with others' feelings\"]\n",
    "                   + full_df[\"Interested in other people's problems\"] # reversed the sign because of reversed scale\n",
    "                   + full_df['Have a soft heart']\n",
    "                   + full_df['Really interested in others']  # reversed the sign because of reversed scale\n",
    "                   + full_df['Take time out for others'] \n",
    "                   + full_df[\"Feel others' emotions\"] \n",
    "                   + full_df['Make people feel at ease']\n",
    "               )\n",
    "\n",
    "# Openness\n",
    "full_df['O'] = (8 + full_df['Have a rich vocabulary']\n",
    "                  - full_df['Have difficulty understanding abstract ideas']\n",
    "                  + full_df['Have a vivid imagination']\n",
    "                  + full_df['Interested in abstract ideas'] # reversed the sign because of reversed scale\n",
    "                  + full_df['Have excellent ideas']\n",
    "                  + full_df['Have a good imagination'] # reversed the sign because of reversed scale\n",
    "                  + full_df['Am quick to understand things']\n",
    "                  + full_df['Use difficult words']\n",
    "                  + full_df['Spend time reflecting on things']\n",
    "                  + full_df['Am full of ideas']\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further understand how these scores relate to each person's purchase behavior, we'll run a correlation analysis. A positive relationship means this category of personality has or wishes to purchase a <strong> Windows </strong> laptop. A negative correlation means this category of personality does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe for correlation matrix\n",
    "corr_df = full_df[dictionary['corr']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Assigning numeric values to purchase behavior\n",
    "corr_df['What laptop would you buy in next assuming if all laptops cost the same?'] = corr_df['What laptop would you buy in next assuming if all laptops cost the same?'].replace(to_replace = ['Windows laptop','Macbook','Chromebook','MAC'],\n",
    "                                                                                           value = [1,-1,-1,-1])\n",
    "\n",
    "# Assigning numeric values to purchase behavior\n",
    "corr_df['What laptop do you currently have?'] = corr_df['What laptop do you currently have?'].replace(to_replace = ['Windows laptop','Macbook','MAC'],\n",
    "                                                                                           value = [1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>What laptop do you currently have?</th>\n",
       "      <th>What laptop would you buy in next assuming if all laptops cost the same?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.019648</td>\n",
       "      <td>-0.046888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>-0.197458</td>\n",
       "      <td>-0.136809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>-0.178324</td>\n",
       "      <td>-0.128762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>-0.014387</td>\n",
       "      <td>-0.035001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>-0.043952</td>\n",
       "      <td>-0.055262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   What laptop do you currently have?  What laptop would you buy in next assuming if all laptops cost the same?\n",
       "A                            0.019648                                                                 -0.046888\n",
       "E                           -0.197458                                                                 -0.136809\n",
       "N                           -0.178324                                                                 -0.128762\n",
       "C                           -0.014387                                                                 -0.035001\n",
       "O                           -0.043952                                                                 -0.055262"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_df.corr().iloc[:5,5:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Extroverts have significance preference for MAC's\n",
    "- Neurotic People have significance preference for MAC's\n",
    "- other personalities don't have particularly strong preferences (observation is aligned with the definition of an agreeable personality)\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Demographic Analysis: </strong> <br>\n",
    "Next we will take a look at the purchase behavior based on respondents gender, ethnicity and program of study. <br> <br>\n",
    "\n",
    "<i> Note: gender in the survey consisted of a binary single choice answer, and did not include non-binary options or a  </i> 'prefer not to answer' <i> option which would have made the survey more inclusive and more accurate (respondents might not have answered with their correct gender and would bias the results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Percent based on Gender #############################################\n",
    "# Subsetting Mac users\n",
    "full_df_Mac      = full_df[full_df['What laptop do you currently have?'] == 'Macbook']\n",
    "\n",
    "# Subsetting Male users of Mac\n",
    "full_df_Mac_male = full_df_Mac[full_df_Mac['Gender'] == 'Male']\n",
    "value_male       = full_df_Mac_male.loc[:,'Gender'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_male_mac =  (value_male / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_male_mac_print = float(percent_male_mac.round(2))\n",
    "\n",
    "\n",
    "\n",
    "# Subsetting Female users of Mac\n",
    "full_df_Mac_female = full_df_Mac[full_df_Mac['Gender'] == 'Female']\n",
    "value_female       = full_df_Mac_female.loc[:,'Gender'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_female_mac =  (value_female / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_female_mac_print = float(percent_female_mac.round(2))\n",
    "\n",
    "\n",
    "# Subsetting users of Windows\n",
    "full_df_windows = full_df[full_df['What laptop do you currently have?'] == 'Windows laptop']\n",
    "\n",
    "# Subsetting Male users of Windows\n",
    "full_df_windows_male = full_df_windows[full_df_windows['Gender'] == 'Male']\n",
    "value_male_win       = full_df_windows_male.loc[:,'Gender'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_male_win =  (value_male_win / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_male_window_print = float(percent_male_win.round(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Subsetting Female users of Windows\n",
    "full_df_windows_female = full_df_windows[full_df_windows['Gender'] == 'Female']\n",
    "value_female_win       = full_df_windows_female.loc[:,'Gender'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_female_windows =  (value_female_win / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_female_win_print = float(percent_female_windows.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gender                 % Macbook Users      % Windows Users\n",
      "-------               ---------------        ----------\n",
      "Male                     26.53                 31.84\n",
      "\n",
      "Female                   24.9                  16.73\n",
      "\n",
      "\n",
      "\n",
      "Ethnicity             % Macbook Users      % Windows Users\n",
      "-----------           ---------------     -------------\n",
      "African American           2.45              2.45\n",
      "\n",
      "Far east Asian             11.43             10.61\n",
      "\n",
      "Hispanic/Latino            6.53              11.43\n",
      "\n",
      "Middle Eastern             0.816             0.816\n",
      "\n",
      "Prefer not to answer       5.31              3.27\n",
      "\n",
      "West Asian / Indian        8.57              12.24\n",
      "\n",
      "White/Caucasian            15.92             7.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################## Percent based on Ethnicity #############################################\n",
    "\n",
    "#African American users of Mac\n",
    "full_df_Mac_African_American = full_df_Mac[full_df_Mac['What is your ethnicity?'] == 'African American']\n",
    "value_African_American       = full_df_Mac_African_American.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_African_American_mac =  (value_African_American / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_African_American_mac_print = float(percent_African_American_mac.round(2))\n",
    "\n",
    "\n",
    "#African American users of Windows\n",
    "full_df_Win_African_American = full_df_windows[full_df_windows['What is your ethnicity?'] == 'African American']\n",
    "value_African_American_win   = full_df_Win_African_American.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_African_American_win =  (value_African_American_win / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_African_American_win_print = float(percent_African_American_win.round(2))\n",
    "\n",
    "\n",
    "#Far east Asian users of Mac\n",
    "full_df_Mac_Far_east_Asian = full_df_Mac[full_df_Mac['What is your ethnicity?'] == 'Far east Asian']\n",
    "value_Far_east_Asian       = full_df_Mac_Far_east_Asian.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_Far_east_Asian_mac =  (value_Far_east_Asian / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_Far_east_Asian_mac_print = float(percent_Far_east_Asian_mac.round(2))\n",
    "\n",
    "\n",
    "#Far east Asian users of Windows\n",
    "full_df_windows_Far_east_Asian = full_df_windows[full_df_windows['What is your ethnicity?'] == 'Far east Asian']\n",
    "value_Far_east_Asian_win       = full_df_windows_Far_east_Asian.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_Far_east_Asian_windows =  (value_Far_east_Asian_win / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_Far_east_Asian_windows_print = float(percent_Far_east_Asian_windows.round(2))\n",
    "\n",
    "\n",
    "#Hispanic / Latino users of Mac\n",
    "full_df_Mac_Hispanic_Latino = full_df_Mac[full_df_Mac['What is your ethnicity?'] == 'Hispanic / Latino']\n",
    "value_Hispanic_Latino_mac   = full_df_Mac_Hispanic_Latino.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_Hispanic_Latino_mac =  (value_Hispanic_Latino_mac / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_Hispanic_Latino_mac_print = float(percent_Hispanic_Latino_mac.round(2))\n",
    "\n",
    "\n",
    "#Hispanic / Latino users of Windows\n",
    "full_df_windows_Hispanic_Latino = full_df_windows[full_df_windows['What is your ethnicity?'] == 'Hispanic / Latino']\n",
    "value_Hispanic_Latino_windows   = full_df_windows_Hispanic_Latino.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_Hispanic_Latino_windows =  (value_Hispanic_Latino_windows / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_Hispanic_Latino_windows_print = float(percent_Hispanic_Latino_windows.round(2))\n",
    "\n",
    "\n",
    "#Middle Eastern users of Mac\n",
    "full_df_Mac_Middle_Eastern = full_df_Mac[full_df_Mac['What is your ethnicity?'] == 'Middle Eastern']\n",
    "value_Middle_Eastern_mac   = full_df_Mac_Middle_Eastern.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_Middle_Eastern_mac =  (value_Middle_Eastern_mac / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_Middle_Eastern_mac_print = float(percent_Middle_Eastern_mac.round(3))\n",
    "\n",
    "\n",
    "#Middle Eastern users of Windows\n",
    "full_df_windows_Middle_Eastern = full_df_windows[full_df_windows['What is your ethnicity?'] == 'Middle Eastern']\n",
    "value_Middle_Eastern_windows   = full_df_windows_Middle_Eastern.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_Middle_Eastern_windows =  (value_Middle_Eastern_windows / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_Middle_Eastern_windows_print = float(percent_Middle_Eastern_windows.round(3))\n",
    "\n",
    "\n",
    "#Prefer not to answer users of Mac\n",
    "full_df_Mac_Prefer_not_to_answer = full_df_Mac[full_df_Mac['What is your ethnicity?'] == 'Prefer not to answer']\n",
    "value_Prefer_not_to_answer_mac   = full_df_Mac_Prefer_not_to_answer.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_Prefer_not_to_answer_mac =  (value_Prefer_not_to_answer_mac / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_Prefer_not_to_answer_mac_print = float(percent_Prefer_not_to_answer_mac.round(2))\n",
    "\n",
    "\n",
    "#Prefer not to answer users of Windows\n",
    "full_df_windows_Prefer_not_to_answer = full_df_windows[full_df_windows['What is your ethnicity?'] == 'Prefer not to answer']\n",
    "value_Prefer_not_to_answer_windows   = full_df_windows_Prefer_not_to_answer.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_Prefer_not_to_answer_windows =  (value_Prefer_not_to_answer_windows / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_Prefer_not_to_answer_windows_print = float(percent_Prefer_not_to_answer_windows.round(2))\n",
    "\n",
    "\n",
    "\n",
    "#West Asian / Indian users of Mac\n",
    "full_df_Mac_West_Asian_Indian = full_df_Mac[full_df_Mac['What is your ethnicity?'] == 'West Asian / Indian']\n",
    "value_West_Asian_Indian_mac   = full_df_Mac_West_Asian_Indian.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_West_Asian_Indian_mac =  (value_West_Asian_Indian_mac / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_West_Asian_Indian_mac_print = float(percent_West_Asian_Indian_mac.round(2))\n",
    "\n",
    "\n",
    "#West Asian / Indian users of Windows\n",
    "full_df_windows_West_Asian_Indian = full_df_windows[full_df_windows['What is your ethnicity?'] == 'West Asian / Indian']\n",
    "value_West_Asian_Indian_windows   = full_df_windows_West_Asian_Indian.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_West_Asian_Indian_windows =  (value_West_Asian_Indian_windows / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_West_Asian_Indian_windows_print = float(percent_West_Asian_Indian_windows.round(2))\n",
    "\n",
    "\n",
    "\n",
    "#White / Caucasian users of Mac\n",
    "full_df_Mac_White_Caucasian = full_df_Mac[full_df_Mac['What is your ethnicity?'] == 'White / Caucasian']\n",
    "value_White_Caucasian_mac   = full_df_Mac_White_Caucasian.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_White_Caucasian_mac =  (value_White_Caucasian_mac / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_White_Caucasian_mac_print = float(percent_White_Caucasian_mac.round(2))\n",
    "\n",
    "\n",
    "#White / Caucasian users of Windows\n",
    "full_df_windows_White_Caucasian = full_df_windows[full_df_windows['What is your ethnicity?'] == 'White / Caucasian']\n",
    "value_White_Caucasian_windows   = full_df_windows_White_Caucasian.loc[:,'What is your ethnicity?'].value_counts()\n",
    "\n",
    "# Calculating percent\n",
    "percent_White_Caucasian_windows =  (value_White_Caucasian_windows / 245) * 100\n",
    "\n",
    "# Formatting for table\n",
    "percent_White_Caucasian_windows_print = float(percent_White_Caucasian_windows.round(2))\n",
    "\n",
    "print(f\"\"\"\n",
    "Gender                 % Macbook Users      % Windows Users\n",
    "-------               ---------------        ----------\n",
    "Male                     {percent_male_mac_print}                 {percent_male_window_print}\n",
    "\n",
    "Female                   {percent_female_mac_print}                  {percent_female_win_print}\n",
    "\n",
    "\n",
    "\n",
    "Ethnicity             % Macbook Users      % Windows Users\n",
    "-----------           ---------------     -------------\n",
    "African American           {percent_African_American_mac_print}              {percent_African_American_win_print}\n",
    "\n",
    "Far east Asian             {percent_Far_east_Asian_mac_print}             {percent_Far_east_Asian_windows_print}\n",
    "\n",
    "Hispanic/Latino            {percent_Hispanic_Latino_mac_print}              {percent_Hispanic_Latino_windows_print}\n",
    "\n",
    "Middle Eastern             {percent_Middle_Eastern_mac_print}             {percent_Middle_Eastern_windows_print}\n",
    "\n",
    "Prefer not to answer       {percent_Prefer_not_to_answer_mac_print}              {percent_Prefer_not_to_answer_windows_print}\n",
    "\n",
    "West Asian / Indian        {percent_West_Asian_Indian_mac_print}              {percent_West_Asian_Indian_windows_print}\n",
    "\n",
    "White/Caucasian            {percent_White_Caucasian_mac_print}             {percent_White_Caucasian_windows_print}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- no significance difference between macbook users but lower percentage of respondents use Windows\n",
    "- no significant conclustions from ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<strong> Churn Analysis: </strong><br><br>\n",
    "\n",
    "In the survey, we asked respondent what are their current laptops and what laptop would they buy assuming all laptops cost the same. Using these two columns, we can determine the churn rate, or in other words, how many customers are switching from Microsoft to Apple. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                      % To Macbook             % To Windows        % To Chrome\n",
      "-------               ---------------        --------------       -------------\n",
      "Churn From Windows        19.33                  77.31                3.36\n",
      "\n",
      "Churn From Mac            89.68                  8.73                 1.59\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WINDOWS\n",
    "full_df_windows      = full_df[full_df['What laptop do you currently have?'] == 'Windows laptop']\n",
    "total_windows        = float(full_df_windows.loc[:,'What laptop do you currently have?'].value_counts())\n",
    "\n",
    "# Windows to Mac churn\n",
    "windows_to_mac       = full_df_windows[full_df_windows['What laptop would you buy in next assuming if all laptops cost the same?'] == 'Macbook']\n",
    "value_windows_to_mac = float(windows_to_mac.loc[:,'What laptop would you buy in next assuming if all laptops cost the same?'].value_counts())\n",
    "\n",
    "# Formatting for table\n",
    "percent_windows_to_mac = round(((value_windows_to_mac / total_windows) * 100),2)\n",
    "\n",
    "\n",
    "# Windows to Windows churn\n",
    "windows_to_windows       = full_df_windows[full_df_windows['What laptop would you buy in next assuming if all laptops cost the same?'] == 'Windows laptop']\n",
    "value_windows_to_windows = float(windows_to_windows.loc[:,'What laptop would you buy in next assuming if all laptops cost the same?'].value_counts())\n",
    "\n",
    "# Formatting for table\n",
    "percent_windows_to_windows = round(((value_windows_to_windows / total_windows) * 100),2)\n",
    "\n",
    "\n",
    "# Windows to Chrome churn\n",
    "windows_to_chrom       = full_df_windows[full_df_windows['What laptop would you buy in next assuming if all laptops cost the same?'] == 'Chromebook']\n",
    "value_windows_to_chrom = float(windows_to_chrom.loc[:,'What laptop would you buy in next assuming if all laptops cost the same?'].value_counts())\n",
    "\n",
    "# Formatting for table\n",
    "percent_windows_to_chrom = round(((value_windows_to_chrom / total_windows) * 100),2)\n",
    "\n",
    "# MACBOOKS\n",
    "full_df_Mac = full_df[full_df['What laptop do you currently have?'] == 'Macbook']\n",
    "total_mac   = float(full_df_Mac.loc[:,'What laptop do you currently have?'].value_counts())\n",
    "\n",
    "# Mac to Mac churn\n",
    "mac_to_mac       = full_df_Mac [full_df_Mac['What laptop would you buy in next assuming if all laptops cost the same?'] == 'Macbook']\n",
    "value_mac_to_mac = float(mac_to_mac.loc[:,'What laptop would you buy in next assuming if all laptops cost the same?'].value_counts())\n",
    "\n",
    "# Formatting for table\n",
    "percent_mac_to_mac = round(((value_mac_to_mac / total_mac) * 100),2)\n",
    "\n",
    "\n",
    "# Mac to Windows churn\n",
    "mac_to_windows       = full_df_Mac[full_df_Mac['What laptop would you buy in next assuming if all laptops cost the same?'] == 'Windows laptop']\n",
    "value_mac_to_windows = float(mac_to_windows.loc[:,'What laptop would you buy in next assuming if all laptops cost the same?'].value_counts())\n",
    "\n",
    "# Formatting for table\n",
    "percent_mac_to_windows = round(((value_mac_to_windows / total_mac) * 100),2)\n",
    "\n",
    "\n",
    "# Mac to Chrome churn\n",
    "mac_to_chrom       = full_df_Mac[full_df_Mac['What laptop would you buy in next assuming if all laptops cost the same?'] == 'Chromebook']\n",
    "value_mac_to_chrom = float(mac_to_chrom.loc[:,'What laptop would you buy in next assuming if all laptops cost the same?'].value_counts())\n",
    "\n",
    "# Formatting for table\n",
    "percent_mac_to_chrom = round(((value_mac_to_chrom / total_mac) * 100),2)\n",
    "\n",
    "print(f\"\"\"\n",
    "                      % To Macbook             % To Windows        % To Chrome\n",
    "-------               ---------------        --------------       -------------\n",
    "Churn From Windows        {percent_windows_to_mac}                  {percent_windows_to_windows}                {percent_windows_to_chrom}\n",
    "\n",
    "Churn From Mac            {percent_mac_to_mac}                  {percent_mac_to_windows}                 {percent_mac_to_chrom}\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- conversion rate to macbooks is higher than to windows -> investigate further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<strong> Variance and Scaling: </strong>\n",
    "<br>\n",
    "Because PCA and Clustering methods are both sensitive to the variance in our data, we want to ensure that every variable and observation are under the same scale. Since this survey was homogenous in terms of the scale of the answers, there is no initial need to scale the data across variables. However, since we are looking at people's behaviors, we know that one person's 5 can actually correspond to another person's 4. Therefore, we will standardize each observation to take this into account in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "As mentioned earlier, the survey consists of 2 different data sources: Hult DNA and Big Five personality test. We will split these into 2 different data frames, without demographic data, to conduct PCA and Clustering Techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping demographic data\n",
    "questions  = full_df.drop(dictionary['demographic'],  axis = 1)\n",
    "\n",
    "\n",
    "# Standardizing per observation with user-defined function\n",
    "questions_rows = standard(questions,      \n",
    "                          by_rows = True)\n",
    "\n",
    "# Standardizing per column\n",
    "questions_cols = standard(questions_rows, \n",
    "                          by_rows = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting between Big Five Questions and Hult DNA Questions\n",
    "big_5    = questions_cols[dictionary['bigfive']]\n",
    "hult_dna = questions_cols[dictionary['hultdna']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build an unsupervised learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Purpose: </strong> <br>\n",
    "The goal of our analysis is to identify potential personality types that could be used in Windows' marketing segmentation strategies. Therefore, principal component analysis, or PCA, is useful to uncover these personalities from our survey respondents' answers since personality is something we cannot measure directly (latent trait exploration).\n",
    "<br><br>\n",
    "***\n",
    "To do so, we will first run a PCA model and then interpret each component's facotr loadings in the next session, which is an interpretable correlation metric. With this, we will be able to identify the different personalities and hult traits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Source 1: Hult DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by conducting a Principal Component Analysis on the questions related to Hult DNA. The goal is to identify certain traits of respondents. <br>\n",
    "The first model does not specify the number of components, since we will use the Scree Plot to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a PCA model\n",
    "pca = PCA(n_components = None,\n",
    "          random_state = 222)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING\n",
    "hult_dna_pca = pca.fit_transform(hult_dna)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#scree_plot(pca, export = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observation: </strong> <br> Best number of components is 5. Upon analysis of factor loadings however, with 5 components, the 2 last features were not significantly clear in terms of customer persona's. Therefore, we selected the analysis with 3 components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a PCA model\n",
    "pca = PCA(n_components = 3,\n",
    "          random_state = 222)\n",
    "\n",
    "# FITTING and TRANSFORMING \n",
    "hult_dna_pca    = pca.fit_transform(hult_dna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Source 2: BIG 5 Personalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a PCA model\n",
    "pca2 = PCA(n_components = None,\n",
    "          random_state = 222)\n",
    "\n",
    "\n",
    "#scree_plot(pca2,export = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observation: </strong> <br> Best number of components is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a PCA model\n",
    "pca2 = PCA(n_components = 5,\n",
    "          random_state = 222)\n",
    "\n",
    "# FITTING and TRANSFORMING \n",
    "big5_pca = pca2.fit_transform(big_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5:  Interpreting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Factor Loadings Analysis: \n",
    "To interpret our principal components, we will analyze its factor loadings, which correspond to how each of the features correlate with each principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hult DNA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# transposing pca components\n",
    "factor_loadings_df  = pd.DataFrame(pd.np.transpose(pca.components_))\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings_df  = factor_loadings_df.set_index(hult_dna.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysis, we have identified 3 personas from the Hult DNA questions:\n",
    "1. Individualistic: works better alone, as opposed to in teams\n",
    "2. Structured Collaborations: connects to others and works collaboratively in team environments\n",
    "3. Visionaries: generates innovative ideas and gather people around them\n",
    "\n",
    "Now that we have developed personas, we can analyze how much each customer fits into each group. Run the following code to view the personas and factor loadings for each customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming each principal component\n",
    "factor_loadings_df.columns  = ['Individualistic',\n",
    "                               'Structured Collaborations',\n",
    "                               'Visionaries']\n",
    "\n",
    "# converting PCA results into a DataFrame \n",
    "hultdna_pca = pd.DataFrame(hult_dna_pca)\n",
    "\n",
    "\n",
    "# renaming columns\n",
    "hultdna_pca.columns = factor_loadings_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Big 5 Personalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# transposing pca components\n",
    "factor_loadings_df2 = pd.DataFrame(pd.np.transpose(pca2.components_))\n",
    "\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings_df2 = factor_loadings_df2.set_index(big_5.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysis, we have identified 5 personas from the Big 5 questions:\n",
    "1. Neuroticism\n",
    "2. Extraversion\n",
    "3. Agrreableness\n",
    "4. Free-spirit\n",
    "5. Conscientiousness\n",
    "\n",
    "Note that we have kept 4 of the 5 original personality types. These matched the factor loadings of our analysis. However, instead of openness, we found that the 5th personality was 'free-spirited'. These are people that do not get stressed often and do not really pay attention to details.<br>\n",
    "Run the following code to view the personas and factor loadings for each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming each principal component\n",
    "factor_loadings_df2.columns = ['Neuroticism',\n",
    "                               'Extraversion',\n",
    "                               'Agreeableness',\n",
    "                               'Free-spirit',\n",
    "                               'Conscientiousness']\n",
    "\n",
    "# converting into a DataFrame \n",
    "big5_pca = pd.DataFrame(big5_pca)\n",
    "\n",
    "# renaming columns\n",
    "big5_pca.columns = factor_loadings_df2.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. Correlation Analysis: \n",
    "As we did with the original big 5 personality traits, we will run a correlation to evaluate each personality's purchase behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# creating a dataframe to join demographic data for correlation\n",
    "corr_df2 = pd.concat([big5_pca,\n",
    "                      full_df[dictionary['demographic']]], \n",
    "                     axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df2['What laptop would you buy in next assuming if all laptops cost the same?'] = \\\n",
    "corr_df2['What laptop would you buy in next assuming if all laptops cost the same?'].replace(to_replace = ['Windows laptop','Macbook','Chromebook','MAC'],\n",
    "                                                                                             value = [1,-1,-1,-1])\n",
    "\n",
    "corr_df2['What laptop do you currently have?'] = \\\n",
    "corr_df2['What laptop do you currently have?'].replace(to_replace = ['Windows laptop','Macbook','MAC'],\n",
    "                                                       value = [1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>What laptop do you currently have?</th>\n",
       "      <th>What laptop would you buy in next assuming if all laptops cost the same?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neuroticism</th>\n",
       "      <td>-0.072583</td>\n",
       "      <td>-0.045011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extraversion</th>\n",
       "      <td>-0.264118</td>\n",
       "      <td>-0.120408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agreeableness</th>\n",
       "      <td>0.096621</td>\n",
       "      <td>0.133254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Free-spirit</th>\n",
       "      <td>0.160850</td>\n",
       "      <td>0.089075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conscientiousness</th>\n",
       "      <td>-0.020654</td>\n",
       "      <td>0.000566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   What laptop do you currently have?  What laptop would you buy in next assuming if all laptops cost the same?\n",
       "Neuroticism                                 -0.072583                                                                 -0.045011\n",
       "Extraversion                                -0.264118                                                                 -0.120408\n",
       "Agreeableness                                0.096621                                                                  0.133254\n",
       "Free-spirit                                  0.160850                                                                  0.089075\n",
       "Conscientiousness                           -0.020654                                                                  0.000566"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_df2.corr().iloc[:5,5:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Further investigating Extraversion correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraverts churning to Macbook\n",
    "len(corr_df2['Extraversion'][corr_df2['Extraversion'] > 0]\\\n",
    "                            [corr_df2['What laptop would you buy in next assuming if all laptops cost the same?'] == 'Macbook'])\n",
    "\n",
    "round((47/245),2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain further insight on our customer groups, we will perform a clustering technique. This will group individuals based on their similarities. If we identify a group that is similar among themselves but somewhat different from others, we can recommend a specific targeted marketing strategy for that group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the clusters, we need to first rescale the data, since we will be using the factor loadings and after analysis, the variance amongst our features is no longer equal.<br><br>\n",
    "Then, we can use the inertia plot and dendrogram to establish the number of clusters we will be using.\n",
    "<br><br> \n",
    "Finally, we will create the clusters using k-Means technique. Let's start with Hult DNA data.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "1. Hult DNA Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scaling our data\n",
    "hultdna_pca_scaled = standard(hultdna_pca, by_rows = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>After analyzing the dendrogram and the inertia plot, we establish that the ideal number of clusters is 4. If we use 4 clusters, we reduce the sum of squares variance by 50% (from 1200 to around 600)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a k-Means object with four clusters\n",
    "hultdna_k_pca = KMeans(n_clusters = 4,\n",
    "                        random_state = 222)\n",
    "\n",
    "\n",
    "# fitting the object to the data\n",
    "hultdna_k_pca.fit(hultdna_pca_scaled)\n",
    "\n",
    "\n",
    "# converting the clusters to a DataFrame\n",
    "hultdna_kmeans_pca = pd.DataFrame({'Cluster': hultdna_k_pca.labels_})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing cluster centers\n",
    "hultdna_centroids_pca = hultdna_k_pca.cluster_centers_\n",
    "\n",
    "\n",
    "# converting cluster centers into a DataFrame\n",
    "hultdna_centroids_pca_df = pd.DataFrame(hultdna_centroids_pca)\n",
    "\n",
    "\n",
    "# renaming principal components\n",
    "hultdna_centroids_pca_df.columns = ['Individualistic',\n",
    "                                    'Structured Collaborations',\n",
    "                                    'Visionaries']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><strong> Concatenating PCA with Demographic Data for Cluster Analysis </strong><br><br>\n",
    "We will later analyze these clusters using box plots and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concat demographic back with hult-dna data\n",
    "hultdna_demo_df = pd.concat([hultdna_kmeans_pca ,\n",
    "                         hultdna_pca],\n",
    "                         axis = 1)\n",
    "\n",
    "final_hultdna_demo_df1 = pd.concat([hultdna_demo_df,\n",
    "                         full_df.loc[:148,dictionary['demographic']]],\n",
    "                         axis = 1,\n",
    "                         join = 'inner')\n",
    "\n",
    "# subsetting to remove missing values from previous duplicates\n",
    "df1 = full_df.loc[295:, dictionary['demographic']].dropna(axis = 1)\n",
    "df2 = df1.tail(n = 97)\n",
    "df2.index = range(148,245)\n",
    "\n",
    "# joining the filtered data \n",
    "final_hultdna_demo_df2      = pd.concat([hultdna_demo_df.iloc[148:,:],\n",
    "                                         df2],\n",
    "                                         axis = 1)\n",
    "final_final_hultdna_demo_df = pd.concat([final_hultdna_demo_df1,\n",
    "                                         final_hultdna_demo_df2],\n",
    "                                         axis = 0)\n",
    "# Naming our clusters\n",
    "hultdna_cluster_names = {0 : 'Cluster 1',\n",
    "                         1 : 'Cluster 2',\n",
    "                         2 : 'Cluster 3',\n",
    "                         3 : 'Cluster 4'}\n",
    "\n",
    "\n",
    "final_final_hultdna_demo_df['Cluster'].replace(hultdna_cluster_names, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Now we plot our boxplots to visually interpret the personalities of each of these clusters as well as the range of personality traits in these clusters.<br><br>\n",
    "<i> Note: the code has been commented out to prevent unnecessary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Box plots for current laptop used\n",
    "#hultdna_lst=['Individualistic','Structured Collaborations','Visionaries']\n",
    "#for i in hultdna_lst:\n",
    "#    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "#    sns.boxplot(x = 'What laptop do you currently have?',\n",
    "#            y = i,\n",
    "#            hue = 'Cluster',\n",
    "#            data = final_final_hultdna_demo_df)\n",
    "#    plt.tight_layout()\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot our boxplots for the future purchase question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Box plots for future purchase\n",
    "#for i in hultdna_lst:\n",
    "#    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "#    sns.boxplot(x = 'What laptop would you buy in next assuming if all laptops cost the same?',\n",
    "#            y = i,\n",
    "#            hue = 'Cluster',\n",
    "#            data = final_final_hultdna_demo_df)\n",
    "#    plt.tight_layout()\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "2. Big 5 Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale our PCA dataframe for more interpretable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scaling our data\n",
    "big5_pca_scaled = standard(big5_pca, by_rows = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>After analyzing the dendrogram and the inertia plot, we establish that the ideal number of clusters is 7. If we use 7 clusters, we reduce the sum of squares variance by 50% (from 2000 to around 1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    65\n",
      "2    39\n",
      "5    36\n",
      "4    35\n",
      "0    28\n",
      "3    27\n",
      "6    15\n",
      "Name: Cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a k-Means object with seven clusters\n",
    "big5_k_pca = KMeans(n_clusters = 7,\n",
    "                        random_state = 222)\n",
    "\n",
    "\n",
    "# fitting the object to the data\n",
    "big5_k_pca.fit(big5_pca_scaled)\n",
    "\n",
    "\n",
    "# converting the clusters to a DataFrame\n",
    "big5_kmeans_pca = pd.DataFrame({'Cluster': big5_k_pca.labels_})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examine our centroids dataframe to understand the balance of personality traits within each of our clusters. This is later used to interpret cluster behavior between current ownership and future purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# storing cluster centers\n",
    "big5_centroids_pca = big5_k_pca.cluster_centers_\n",
    "\n",
    "\n",
    "# converting cluster centers into a DataFrame\n",
    "big5_centroids_pca_df = pd.DataFrame(big5_centroids_pca)\n",
    "\n",
    "\n",
    "# renaming principal components\n",
    "big5_centroids_pca_df.columns = ['Neuroticism',\n",
    "                                 'Extraversion',\n",
    "                                 'Agreeableness',\n",
    "                                 'Free-spirit',\n",
    "                                 'Conscientiousness']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat demographic back with hult-dna data\n",
    "big5_demo_df = pd.concat([big5_kmeans_pca ,\n",
    "                          big5_pca],\n",
    "                          axis = 1)\n",
    "\n",
    "final_big5_demo_df1 = pd.concat([big5_demo_df,\n",
    "                                 full_df.loc[:148,dictionary['demographic']]],\n",
    "                                 axis = 1,\n",
    "                                 join='inner')\n",
    "\n",
    "# subsetting to remove missing values from previous duplicates\n",
    "df1 = full_df.loc[295:,dictionary['demographic']].dropna(axis = 1)\n",
    "df2 = df1.tail(n = 97)\n",
    "df2.index = range(148,245)\n",
    "\n",
    "# joining the filtered data \n",
    "final_big5_demo_df2      = pd.concat([big5_demo_df.iloc[148:,:],\n",
    "                                      df2],\n",
    "                                      axis = 1)\n",
    "final_final_big5_demo_df = pd.concat([final_big5_demo_df1,\n",
    "                                      final_big5_demo_df2],\n",
    "                                      axis = 0)\n",
    "\n",
    "# Naming our clusters\n",
    "big5_cluster_names = {0 : 'Cluster 1',\n",
    "                      1 : 'Cluster 2',\n",
    "                      2 : 'Cluster 3',\n",
    "                      3 : 'Cluster 4',\n",
    "                      4 : 'Cluster 5',\n",
    "                      5 : 'Cluster 6',\n",
    "                      6 : 'Cluster 7'}\n",
    "\n",
    "\n",
    "final_final_big5_demo_df['Cluster'].replace(big5_cluster_names, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "For each question, we assign counts of Macbook and PC to different objects. We then print these objects in a dynamic string for interpretation of consumer migration between platforms for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Cluster 6 Cluster 7\n",
      "            \n",
      "Mac Owner:     19         37         19        18         11         14        8\n",
      "\n",
      "Windows Owner:9         28         20         9         24         22         7\n",
      "__________________________________________________________________________________\n",
      "\n",
      "Mac Buyer:    19         42         22        14         16         15        8\n",
      "\n",
      "Windows Buyer:8         23         16         12         18         19         7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C1_Macowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 1'][final_final_big5_demo_df['What laptop do you currently have?']=='Macbook'].count()\n",
    "C2_Macowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 2'][final_final_big5_demo_df['What laptop do you currently have?']=='Macbook'].count()\n",
    "C3_Macowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 3'][final_final_big5_demo_df['What laptop do you currently have?']=='Macbook'].count()\n",
    "C4_Macowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 4'][final_final_big5_demo_df['What laptop do you currently have?']=='Macbook'].count()\n",
    "C5_Macowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 5'][final_final_big5_demo_df['What laptop do you currently have?']=='Macbook'].count()\n",
    "C6_Macowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 6'][final_final_big5_demo_df['What laptop do you currently have?']=='Macbook'].count()\n",
    "C7_Macowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 7'][final_final_big5_demo_df['What laptop do you currently have?']=='Macbook'].count()\n",
    "\n",
    "C1_Windowsowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 1'][final_final_big5_demo_df['What laptop do you currently have?']=='Windows laptop'].count()\n",
    "C2_Windowsowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 2'][final_final_big5_demo_df['What laptop do you currently have?']=='Windows laptop'].count()\n",
    "C3_Windowsowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 3'][final_final_big5_demo_df['What laptop do you currently have?']=='Windows laptop'].count()\n",
    "C4_Windowsowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 4'][final_final_big5_demo_df['What laptop do you currently have?']=='Windows laptop'].count()\n",
    "C5_Windowsowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 5'][final_final_big5_demo_df['What laptop do you currently have?']=='Windows laptop'].count()\n",
    "C6_Windowsowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 6'][final_final_big5_demo_df['What laptop do you currently have?']=='Windows laptop'].count()\n",
    "C7_Windowsowners = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 7'][final_final_big5_demo_df['What laptop do you currently have?']=='Windows laptop'].count()\n",
    "\n",
    "C1_Macbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 1'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Macbook'].count()\n",
    "C2_Macbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 2'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Macbook'].count()\n",
    "C3_Macbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 3'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Macbook'].count()\n",
    "C4_Macbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 4'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Macbook'].count()\n",
    "C5_Macbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 5'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Macbook'].count()\n",
    "C6_Macbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 6'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Macbook'].count()\n",
    "C7_Macbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 7'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Macbook'].count()\n",
    "\n",
    "C1_Windowsbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 1'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Windows laptop'].count()\n",
    "C2_Windowsbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 2'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Windows laptop'].count()\n",
    "C3_Windowsbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 3'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Windows laptop'].count()\n",
    "C4_Windowsbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 4'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Windows laptop'].count()\n",
    "C5_Windowsbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 5'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Windows laptop'].count()\n",
    "C6_Windowsbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 6'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Windows laptop'].count()\n",
    "C7_Windowsbuyers = final_final_big5_demo_df['Cluster'][final_final_big5_demo_df['Cluster']=='Cluster 7'][final_final_big5_demo_df['What laptop would you buy in next assuming if all laptops cost the same?']=='Windows laptop'].count()\n",
    "\n",
    "print(f\"\"\"\n",
    "            Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Cluster 6 Cluster 7\n",
    "            \n",
    "Mac Owner:     {C1_Macowners}         {C2_Macowners}         {C3_Macowners}        {C4_Macowners}         {C5_Macowners}         {C6_Macowners}        {C7_Macowners}\n",
    "\n",
    "Windows Owner:{C1_Windowsowners}         {C2_Windowsowners}         {C3_Windowsowners}         {C4_Windowsowners}         {C5_Windowsowners}         {C6_Windowsowners}         {C7_Windowsowners}\n",
    "__________________________________________________________________________________\n",
    "\n",
    "Mac Buyer:    {C1_Macbuyers}         {C2_Macbuyers}         {C3_Macbuyers}        {C4_Macbuyers}         {C5_Macbuyers}         {C6_Macbuyers}        {C7_Macbuyers}\n",
    "\n",
    "Windows Buyer:{C1_Windowsbuyers}         {C2_Windowsbuyers}         {C3_Windowsbuyers}         {C4_Windowsbuyers}         {C5_Windowsbuyers}         {C6_Windowsbuyers}         {C7_Windowsbuyers}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Now we plot our boxplots for both questions to better interpret these clusters.\n",
    "<br><br>\n",
    "<i> Note: the code has been commented out to prevent unnecessary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big5_lst=['Neuroticism', 'Extraversion', 'Agreeableness', 'Free-spirit', 'Conscientiousness']\n",
    "#for i in big5_lst:\n",
    "#    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "#    sns.boxplot(x = 'What laptop do you currently have?',\n",
    "#            y = i,\n",
    "#            hue = 'Cluster',\n",
    "#            data = final_final_big5_demo_df)\n",
    "#    plt.tight_layout()\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big5_lst=['Neuroticism', 'Extraversion', 'Agreeableness', 'Free-spirit', 'Conscientiousness']\n",
    "#for i in big5_lst:\n",
    "#    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "#    sns.boxplot(x = 'What laptop would you buy in next assuming if all laptops cost the same?',\n",
    "#                y = i,\n",
    "#                hue = 'Cluster',\n",
    "#                data = final_final_big5_demo_df)\n",
    "#    plt.tight_layout()\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- Large percentages of windows owners which are in clusters 2 (calm and tranquil extraverted individuals), 3 (people with the personality trait of being careful or diligent) and 5 (people with kind, sympathetic, cooperative personalities), intend to switch to MacBook computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In an international crowd, not all personalities are represented by the Big 5\n",
    "- Microsoft is currently losing to Apple in the extroverts’ market for laptops. Of our respondents, 20% are extroverts that would choose to buy a Macbook for their next laptop.\n",
    "- A group of people want to move from Mac to Windows. This group includes personalities that enjoy being around people, participating in social gatherings, and are full of energy. They are an opportunity for Microsoft to gain new customers by tapping this niche market with targeted advertising themed around social gatherings and connections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "310px",
    "width": "359px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
